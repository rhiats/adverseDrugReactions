{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "15DL5TZ8KCnka9QwcFaEux50ceq_nlu9x",
      "authorship_tag": "ABX9TyNbAWmZcnvzOUrNTUIeJaad"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Medical Case Reports and Biomedical Literature**\n",
        "\n",
        "PHEE Datasource 5,000 annotated events from medical case reports and biomedical literature, making it the largest such public dataset to date. \n",
        "\n",
        "The dataset is extracted from MEDLINE case reports and each sentence features two levels of annotations: coarse-grained and fine-grained.  \n",
        "\n",
        "*   The coarse-grained annotations contain event trigger word/phrase,event type and text spans indicating the eventâ€™s associated subject, treatment, and effect.\n",
        "*   The fine-grained annotations contain patient demographic information, the context information about the treatments including drug dosage levels, administration routes, frequency, and attributes relating to events (Sun, 2022).\n",
        "\n",
        "The train, development and test sets are to be recreated with stratified sampling using the demographic information, then try three models: Sequence Labeling, Extractive QA, Generative QA to classify pertinent information in the dataset."
      ],
      "metadata": {
        "id": "qmQ0dLFwmTle"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import os\n",
        "import glob"
      ],
      "metadata": {
        "id": "n-n_U3cymUOL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Parse Data Files**"
      ],
      "metadata": {
        "id": "HKJueZ47w_Ew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def concat_txt_file(input_path,output_path):\n",
        "  \"\"\"\n",
        "    Parse the text files and save the data to .pkl file\n",
        "\n",
        "    @p:\n",
        "    input_path (str): Path of the directory with input data\n",
        "    output_path (str): Path of the directory to save pkl file\n",
        "\n",
        "  \"\"\"\n",
        "  \n",
        "  temp_dict={'id':[],'text':[]}\n",
        "\n",
        "  for file_name in glob.glob(input_path + \"/*.txt\"):\n",
        "    id=file_name.split('.')[0].split('/')[-1]\n",
        "    temp_dict['id'].append(id)\n",
        "\n",
        "    with open(file_name) as f:\n",
        "      lines=f.readlines()[0]\n",
        "    \n",
        "    temp_dict['text'].append(lines)\n",
        "\n",
        "    with open(output_path, 'wb') as handle:\n",
        "      pickle.dump(temp_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "def concat_ann_file(input_path,output_path):\n",
        "  \"\"\"\n",
        "    Parse the ann files and save the data to .pkl file\n",
        "\n",
        "    @p:\n",
        "    input_path (str): Path of the directory with input data\n",
        "    output_path (str): Path of the directory to save pkl file\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  temp_dict={'id':[],'Subject':[],'Treatment':[],\n",
        "             'Potential_therapeutic_event':[],\n",
        "             'Drug':[],'Effect':[],'Adverse_event':[],\n",
        "             'Race':[],'Age':[],'Gender':[],\n",
        "             'Population':[],'Disorder':[],'Duration':[],\n",
        "             'Time_elapsed':[],'Route':[],'Freq':[],\n",
        "             'Dosage':[],'Combination.Drug':[]}\n",
        "\n",
        "  label_lst=['Subject','Treatment',\n",
        "             'Potential_therapeutic_event',\n",
        "             'Drug','Effect','Adverse_event',\n",
        "             'Race','Age','Gender',\n",
        "             'Population','Disorder','Duration',\n",
        "             'Time_elapsed','Route','Freq',\n",
        "             'Dosage','Combination.Drug']\n",
        "\n",
        "  for file_name in glob.glob(input_path + \"/*.ann\")[0:10]:\n",
        "    id=file_name.split('.')[0].split('/')[-1]\n",
        "\n",
        "    temp_dict['id'].append(id)\n",
        "\n",
        "    ck_cnt=0\n",
        "    \n",
        "\n",
        "    with open(file_name, 'r') as document_anno_file:\n",
        "      lines = document_anno_file.readlines()\n",
        "      \n",
        "      s, t, p, d, e, a, r, ag, g, pop, dis, dur, t_e, rout, freq, dos, com = ([] for i in range(17))\n",
        "\n",
        "      temp_arr_lst=[s, t, p, d, e, a, r, ag, g, pop, dis, dur, t_e, rout, freq, dos, com]\n",
        "\n",
        "      for line in lines:\n",
        "        standoff_line=line.split(\"\\t\")\n",
        "        standoff_line.pop(0)\n",
        "\n",
        "        identifier=standoff_line[0].split()[0].strip()\n",
        "        \n",
        "        if identifier=='Drug': d.append(standoff_line[1].strip())\n",
        "        elif identifier=='Effect': e.append(standoff_line[1].strip())\n",
        "        elif identifier=='Adverse_event': a.append(standoff_line[1].strip())\n",
        "        elif identifier=='Subject': s.append(standoff_line[1].strip())\n",
        "        elif identifier=='Treatment': t.append(standoff_line[1].strip())\n",
        "        elif identifier=='Potential_therapeutic_event': p.append(standoff_line[1].strip())\n",
        "        elif identifier=='Race': r.append(standoff_line[1].strip())\n",
        "        elif identifier=='Age': ag.append(standoff_line[1].strip())\n",
        "        elif identifier=='Gender': g.append(standoff_line[1].strip())\n",
        "        elif identifier=='Population': pop.append(standoff_line[1].strip())\n",
        "        elif identifier=='Disorder': dis.append(standoff_line[1].strip())\n",
        "        elif identifier=='Duration': dur.append(standoff_line[1].strip())\n",
        "        elif identifier=='Time_elapsed': t_e.append(standoff_line[1].strip())\n",
        "        elif identifier=='Route': rout.append(standoff_line[1].strip())\n",
        "        elif identifier=='Freq': freq.append(standoff_line[1].strip())\n",
        "        elif identifier=='Dosage': dos.append(standoff_line[1].strip())\n",
        "        elif identifier=='Combination.Drug': com.append(standoff_line[1].strip())\n",
        "\n",
        "        for i in range(len(label_lst)):\n",
        "          temp_dict[label_lst[i]].append(temp_arr_lst[i])\n",
        "\n",
        "    with open(output_path, 'wb') as handle:\n",
        "      pickle.dump(temp_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
      ],
      "metadata": {
        "id": "9kcu4QJzH_L_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_train='/content/drive/MyDrive/PHEE/input/clean/train'\n",
        "path_dev='/content/drive/MyDrive/PHEE/input/clean/dev'\n",
        "path_test='/content/drive/MyDrive/PHEE/input/clean/test'\n",
        "\n",
        "parse_PHEE_data.concat_txt_file(path_train,'/content/drive/MyDrive/PHEE/output/train_txt.pickle')\n",
        "concat_txt_file(path_dev,'/content/drive/MyDrive/PHEE/output/dev_txt.pickle')\n",
        "concat_txt_file(path_test,'/content/drive/MyDrive/PHEE/output/test_txt.pickle')\n",
        "concat_ann_file(path_train,'/content/drive/MyDrive/PHEE/output/train_ann.pickle')\n",
        "concat_ann_file(path_dev,'/content/drive/MyDrive/PHEE/output/dev_ann.pickle')\n",
        "concat_ann_file(path_test,'/content/drive/MyDrive/PHEE/output/test_ann.pickle')"
      ],
      "metadata": {
        "id": "5uqHFlq3wYBn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "outputId": "0667e3f4-8c91-4b44-c72e-4b1aef81cb7f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-154f1afafcde>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpath_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/drive/MyDrive/PHEE/input/clean/test'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mparse_PHEE_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat_txt_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'/content/drive/MyDrive/PHEE/output/train_txt.pickle'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mconcat_txt_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_dev\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'/content/drive/MyDrive/PHEE/output/dev_txt.pickle'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mconcat_txt_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'/content/drive/MyDrive/PHEE/output/test_txt.pickle'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'parse_PHEE_data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load Parsed Data Files**"
      ],
      "metadata": {
        "id": "asvn11PT3orK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_name_lst=['/content/drive/MyDrive/PHEE/output/train_txt.pickle',\n",
        "'/content/drive/MyDrive/PHEE/output/dev_txt.pickle',\n",
        "'/content/drive/MyDrive/PHEE/output/test_txt.pickle',\n",
        "'/content/drive/MyDrive/PHEE/output/train_ann.pickle',\n",
        "'/content/drive/MyDrive/PHEE/output/dev_ann.pickle',\n",
        "'/content/drive/MyDrive/PHEE/output/test_ann.pickle']\n",
        "\n",
        "for f in file_name_lst:\n",
        "  with open(f, 'rb') as handle:\n",
        "      d = pickle.load(handle)\n",
        "      print(len(d))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLTBzdGR2_2y",
        "outputId": "cbacc57a-b455-422d-b744-ccad47f05298"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "2\n",
            "2\n",
            "18\n",
            "18\n",
            "18\n"
          ]
        }
      ]
    }
  ]
}